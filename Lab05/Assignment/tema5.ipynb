{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff150e46-8229-49ed-9394-5dbcd425e42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 13:38:56.479549: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-06 13:38:56.531806: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-06 13:38:56.531852: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-06 13:38:56.531890: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-06 13:38:56.543303: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-06 13:38:57.717290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import freeze_support\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from sam import SAM\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435e5c60-54b8-4350-b468-3ce6601f0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'tema5.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a9ec26-1a10-475b-8fc3-cc9ea1c3f9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise\n",
    "        # (cuda:1).\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mos')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6148311e-03c6-47a3-94d1-8fa48783aa62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    train_transforms = [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        # v2.RandomAffine(degrees=10, translate=(0.0, 0.1), scale=(0.8, 1.0)),\n",
    "        # v2.RandomHorizontalFlip(0.25),\n",
    "        # v2.RandomVerticalFlip(0.25),\n",
    "        v2.Normalize(mean=[0.49139968, 0.48215827, 0.44653124], std=[0.24703233, 0.24348505, 0.26158768]),\n",
    "        v2.Resize((28, 28), antialias=True),\n",
    "        v2.Grayscale(),\n",
    "        torch.flatten,\n",
    "    ]\n",
    "    valid_transforms = [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.49139968, 0.48215827, 0.44653124], std=[0.24703233, 0.24348505, 0.26158768]),\n",
    "        v2.Resize((28, 28), antialias=True),\n",
    "        v2.Grayscale(),\n",
    "        torch.flatten,\n",
    "    ]\n",
    "    \n",
    "    return train_transforms, valid_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "114d19c8-72db-4d6b-bf77-ec5a38f2f2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CachedDataset(Dataset):\n",
    "    def __init__(self, dataset, cache=True):\n",
    "        if cache:\n",
    "            dataset = tuple([x for x in dataset])\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c079bf-cd04-4621-8242-8ac4e541a956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(train_transforms, valid_transforms):\n",
    "    data_path = './data'\n",
    "    train_dataset = CIFAR10(root=data_path, train=True, transform=v2.Compose(train_transforms), download=False)\n",
    "    val_dataset = CIFAR10(root=data_path, train=False, transform=v2.Compose(valid_transforms), download=False)\n",
    "    train_dataset = CachedDataset(train_dataset, cache=True)\n",
    "    val_dataset = CachedDataset(val_dataset, cache=True)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44797bd2-baee-4ce4-8d8b-efd798c93a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dropout_prob = 0.2\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size * 2, hidden_size * 4)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size * 4)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size * 4, hidden_size * 2)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size * 2)\n",
    "\n",
    "        self.fc5 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.fc6 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for fct, bn in zip([self.fc1, self.fc2, self.fc3, self.fc4, self.fc5], [self.bn1, self.bn2, self.bn3, self.bn4, self.bn5]):\n",
    "            x = fct(x)\n",
    "            x = bn(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb26eb6-d492-4ec3-873a-b7225c5f99c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(labels, output):\n",
    "    return (torch.argmax(output, axis=1) == labels).sum().item() / len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f15d77f-a954-45c7-90a4-a997cfbe52c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, writer, optimizator):\n",
    "    model.train()\n",
    "\n",
    "    acc_train = 0.0\n",
    "    loss_train = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for data, labels in train_loader:\n",
    "        batch += 1\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss_train += loss.item()\n",
    "        writer.add_scalar(\"Loss/Training/Batch\", batch, loss.item())\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        acc_train += accuracy(labels, output)\n",
    "\n",
    "    return loss_train / batch, round(acc_train / len(train_loader), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce72e8e-0112-4bff-a78c-2473513fdb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def val(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    acc_val = 0.0\n",
    "    loss_valid = 0.0\n",
    "\n",
    "    for data, labels in val_loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "            loss_valid += loss.item()\n",
    "\n",
    "        acc_val += accuracy(labels, output)\n",
    "\n",
    "    return loss_valid / len(val_loader), round(acc_val / len(val_loader), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07958b77-2fd4-4281-8012-2f5a771f0f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_epoch(model, train_loader, val_loader, criterion, optimizer, device, writer, optimizator):\n",
    "    loss_train, acc = train(model, train_loader, criterion, optimizer, device, writer, optimizator)\n",
    "    loss_valid, acc_val = val(model, val_loader, criterion, device)\n",
    "\n",
    "    return loss_train, acc, loss_valid, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "940575ea-1c98-4f23-9184-ab9c26de3062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_norm(model):\n",
    "    norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        norm += torch.norm(param)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf86c673-0151-4639-bd97-1fa9308a2926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def poly_lr(ep, max_ep, initial_lr, exponent=0.9):\n",
    "    return initial_lr * (1 - ep / max_ep)**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83a0b21c-ef98-46e8-b00c-f09ee12c3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_global = 0.0\n",
    "best_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "631dffc8-6427-4a58-97fe-a5abc78ac908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_train():\n",
    "    wandb.init(\n",
    "        project=\"cifar-10\",\n",
    "\n",
    "        config={\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    batch_size = wandb.config.batch_size\n",
    "    epochs = wandb.config.epochs\n",
    "    optimizator = wandb.config.optimizer\n",
    "    learning_rate = wandb.config.learning_rate\n",
    "    make_poly_lr = wandb.config.poly_lr\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    writer.add_scalar(\"Batch_size\", batch_size)\n",
    "    writer.add_scalar(\"Learning_rate\", learning_rate)\n",
    "    writer.add_scalar(\"Optimizer\", optimizator)\n",
    "    \n",
    "    device = get_default_device()\n",
    "    \n",
    "    train_transforms, valid_transforms = get_transforms()\n",
    "    train_dataset, val_dataset = get_dataset(train_transforms, valid_transforms)\n",
    "\n",
    "    pin_memory = device.type == 'cuda'\n",
    "    num_workers = 2\n",
    "    persistent_workers = (num_workers != 0)\n",
    "    val_batch_size = 500\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=pin_memory, num_workers=num_workers,\n",
    "                            batch_size=batch_size, drop_last=True, persistent_workers=persistent_workers)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
    "                            drop_last=False)\n",
    "    \n",
    "    model = MLP(784, 1000, 10)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if optimizator == 0:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "    elif optimizator == 1:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=3e-05, nesterov=True)\n",
    "    elif optimizator == 2:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.9)\n",
    "    elif optimizator == 3:\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        base_optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=3e-05, nesterov=True)\n",
    "        optimizer = SAM(model.parameters(), base_optimizer, rho=0.05)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    besst = 0\n",
    "    \n",
    "    tbar = tqdm(tuple(range(epochs)))\n",
    "    for epoch in tbar:\n",
    "        if make_poly_lr == 1:\n",
    "            for g in optimizer.param_groups:\n",
    "                lr = poly_lr(epoch, epochs, learning_rate, 0.9)\n",
    "                g['lr'] = lr\n",
    "\n",
    "        loss_train, acc, loss_valid, acc_val = do_epoch(model, train_loader, val_loader, criterion, optimizer, device, writer, optimizator)\n",
    "        tbar.set_postfix_str(f\"Acc: {acc}, Acc_val: {acc_val}\")\n",
    "        \n",
    "        writer.add_scalar(\"Train/Accuracy\", acc, epoch)\n",
    "        writer.add_scalar(\"Train/Loss\", loss_train, epoch)\n",
    "        writer.add_scalar(\"Val/Accuracy\", acc_val, epoch)\n",
    "        writer.add_scalar(\"Val/Loss\", loss_valid, epoch)\n",
    "        \n",
    "        writer.add_scalar(\"Model/Norm\", get_model_norm(model), epoch)\n",
    "        if make_poly_lr == 1:\n",
    "            writer.add_scalar(\"Learning_rate/Epoch\", g['lr'], epoch)\n",
    "        \n",
    "        wandb.log({\"acc\": acc, \"acc_val\": acc_val})\n",
    "\n",
    "        if besst < acc_val:\n",
    "            besst = acc_val\n",
    "    \n",
    "    json_file_path = \"sweep.json\"\n",
    "    new_data = {\"Acc Val\": besst, \"config\": wandb.config.as_dict()}\n",
    "    try:\n",
    "        with open(json_file_path, \"r\") as json_file:\n",
    "            existing_data = json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = {}\n",
    "\n",
    "    new_key = f\"entry_{len(existing_data)}\"\n",
    "    existing_data[new_key] = new_data\n",
    "    \n",
    "    with open(json_file_path, \"w\") as json_file:\n",
    "        json.dump(existing_data, json_file, indent=4)\n",
    "            \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6b5508-9f9b-4695-89d7-a96327da6489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'project': 'cifar-10',\n",
    "    'parameters': {\n",
    "        \"batch_size\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 256,\n",
    "            \"max\": 512\n",
    "        },\n",
    "        'epochs': {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 75,\n",
    "            \"max\": 125\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [0, 1, 2, 3, 4]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4]\n",
    "        },\n",
    "        \"poly_lr\": {\n",
    "            \"values\": [0, 1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8fe76-2757-46c4-8ddc-ec2abd5cc7e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ob6iump8\n",
      "Sweep URL: https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8g6snkq1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpoly_lr: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrazvanpanaite\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/P36_LIVER/users/razvan/scripts/Faculty/RN/Lab05/wandb/run-20231106_133904-8g6snkq1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/razvanpanaite/cifar-10/runs/8g6snkq1' target=\"_blank\">winter-sweep-1</a></strong> to <a href='https://wandb.ai/razvanpanaite/cifar-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/razvanpanaite/cifar-10' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/razvanpanaite/cifar-10/runs/8g6snkq1' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10/runs/8g6snkq1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.21s/it, Acc: 0.9309, Acc_val: 0.5159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entry_0': {'Acc Val': 0.5194, 'config': {'batch_size': 480, 'epochs': 100, 'learning_rate': 0.0005, 'optimizer': 1, 'poly_lr': 0, 'dataset': 'CIFAR-10'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8698431f71014fd79d64efa7b55d34cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>acc_val</td><td>▁▄▅▆▆▇▇▇▇▇▇▇▇██▇████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.9309</td></tr><tr><td>acc_val</td><td>0.5159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-1</strong> at: <a href='https://wandb.ai/razvanpanaite/cifar-10/runs/8g6snkq1' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10/runs/8g6snkq1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231106_133904-8g6snkq1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7fi27hx5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpoly_lr: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/P36_LIVER/users/razvan/scripts/Faculty/RN/Lab05/wandb/run-20231106_134157-7fi27hx5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/razvanpanaite/cifar-10/runs/7fi27hx5' target=\"_blank\">misty-sweep-2</a></strong> to <a href='https://wandb.ai/razvanpanaite/cifar-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/razvanpanaite/cifar-10' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10/sweeps/ob6iump8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/razvanpanaite/cifar-10/runs/7fi27hx5' target=\"_blank\">https://wandb.ai/razvanpanaite/cifar-10/runs/7fi27hx5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████▊                             | 60/101 [01:38<01:07,  1.64s/it, Acc: 0.9091, Acc_val: 0.5072]"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='cifar-10')\n",
    "wandb.agent(sweep_id, function=main_train, count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595129cb-5882-4b98-946b-97f2f58a3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = \"sweep.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    existing_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa7b553-27e0-49b5-a95d-ce8b2bf3555f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5365\n"
     ]
    }
   ],
   "source": [
    "maxim = 0\n",
    "for it in existing_data:\n",
    "    maxim = max(maxim, existing_data[it]['Acc Val'])\n",
    "print(maxim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3ada5-1aaa-435e-ba41-404e6b209d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
